# GridWorld RL - Explainable AI with DQN

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Accuracy](https://img.shields.io/badge/Accuracy-85%25-green)
![Framework](https://img.shields.io/badge/Framework-Stable--Baselines3-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ¯ Description du Projet

SystÃ¨me d'apprentissage par renforcement (RL) avec agent DQN naviguant dans un environnement GridWorld 5Ã—5. L'agent apprend Ã  Ã©viter des piÃ¨ges positionnÃ©s alÃ©atoirement pour atteindre un objectif fixe. Le projet inclut des explications gÃ©nÃ©rÃ©es par LLM (Gemini 2.5 Flash) et une interface interactive Streamlit.

### CaractÃ©ristiques Principales

- âœ… Agent DQN entraÃ®nÃ© avec reward shaping avancÃ©
- âœ… PÃ©nalitÃ©s de proximitÃ© aux obstacles pour Ã©vitement anticipÃ©
- âœ… Interface web interactive Streamlit
- âœ… Visualisations Plotly (trajectoires, Q-values, rÃ©compenses)
- âœ… Explications LLM avec dÃ©tection automatique d'hallucinations
- âœ… Support dynamique de 1 Ã  5 piÃ¨ges

## ğŸ“Š Performances du ModÃ¨le

### MÃ©triques Globales

- **Accuracy finale:** 85.0% (17/20 succÃ¨s avec 3 piÃ¨ges)
- **OptimalitÃ©:** 100% des succÃ¨s utilisent le chemin optimal (8 pas)
- **EntraÃ®nement:** 1.2M timesteps en 18-20 minutes sur CPU
- **Vitesse:** ~3000 frames/seconde

### Performance par DifficultÃ©

| Nombre de PiÃ¨ges | Taux de SuccÃ¨s |
|------------------|----------------|
| 1 piÃ¨ge          | ~95%           |
| 2 piÃ¨ges         | ~90%           |
| 3 piÃ¨ges         | 85%            |
| 4 piÃ¨ges         | ~75%           |
| 5 piÃ¨ges         | ~65%           |

### MÃ©triques d'EntraÃ®nement (TensorBoard)

- RÃ©compense moyenne: -30 (dÃ©but) â†’ +60-70 (convergence)
- Longueur Ã©pisodes: 11 pas (dÃ©but) â†’ 7-8 pas (optimal)
- Loss rÃ©seau: 2.5-5.0 (stable, pas de divergence)

## ğŸ› ï¸ Stack Technique

### Apprentissage par Renforcement
- **Algorithme:** Deep Q-Network (DQN)
- **Framework RL:** Stable-Baselines3 2.0.0
- **Environnement:** Gymnasium 0.28.1
- **Deep Learning:** PyTorch 2.0.1

### Interface et Visualisation
- **Interface Web:** Streamlit 1.28.0
- **Graphiques Interactifs:** Plotly 5.17.0
- **Monitoring:** TensorBoard

### Explications IA
- **LLM:** Google Gemini 2.5 Flash
- **Validation:** SystÃ¨me de dÃ©tection d'hallucinations custom

### Utilitaires
- **Calculs:** NumPy 1.24.3
- **Langage:** Python 3.10

## ğŸ“¦ Installation

### PrÃ©requis

- Python 3.10 ou supÃ©rieur
- pip (gestionnaire de paquets Python)
- Compte Google Cloud (pour API Gemini)

### Installation Rapide

```bash
# 1. Cloner le repository
git clone https://github.com/VOTRE_USERNAME/gridworld-rl-explainable.git
cd gridworld-rl-explainable

# 2. CrÃ©er environnement virtuel
python -m venv venv

# 3. Activer l'environnement
# Windows
.\venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# 4. Installer les dÃ©pendances
pip install -r requirements.txt

# 5. Configurer l'API Gemini
# CrÃ©er fichier .env Ã  la racine
echo "GOOGLE_API_KEY=votre_cle_api_ici" > .env
```

### Obtenir une clÃ© API Gemini

1. Aller sur [Google AI Studio](https://makersuite.google.com/app/apikey)
2. CrÃ©er une nouvelle clÃ© API
3. Copier la clÃ© dans le fichier `.env`

## ğŸ® Utilisation

### 1. EntraÃ®ner le ModÃ¨le

```bash
python src/agents/train_dqn.py
```

**ParamÃ¨tres modifiables dans le fichier:**
- `total_timesteps`: 1 200 000 (dÃ©faut)
- `learning_rate`: 0.0007
- `num_traps`: 1-3 piÃ¨ges randomisÃ©s

**DurÃ©e:** ~18-20 minutes sur CPU moderne

### 2. Tester le ModÃ¨le

```bash
# Test standard (20 Ã©pisodes, 3 piÃ¨ges)
python src/agents/test_agent.py --episodes 20

# Test personnalisÃ©
python src/agents/test_agent.py --episodes 30 --render
```

**Options disponibles:**
- `--episodes`: Nombre d'Ã©pisodes Ã  tester
- `--render`: Activer le rendu visuel
- `--analyze`: Afficher analyse dÃ©taillÃ©e des Q-values

### 3. Lancer l'Application Streamlit

```bash
streamlit run app/streamlit_app.py
```

**AccÃ¨s:** http://localhost:8501

**FonctionnalitÃ©s:**
- Slider pour choisir nombre de piÃ¨ges (1-5)
- GÃ©nÃ©ration d'Ã©pisodes alÃ©atoires
- Navigation dans historique d'Ã©pisodes
- 3 visualisations interactives Plotly
- Explications LLM pour premier pas

### 4. Visualiser l'EntraÃ®nement (TensorBoard)

```bash
tensorboard --logdir=logs/tensorboard
```

**AccÃ¨s:** http://localhost:6006

**MÃ©triques disponibles:**
- RÃ©compense moyenne par Ã©pisode
- Longueur moyenne des Ã©pisodes
- Loss du rÃ©seau Q
- Taux d'exploration (epsilon)

## ğŸ“ Structure du Projet

```
gridworld_llm_project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ gridworld.py              # Environnement GridWorld custom
â”‚   â”‚   â””â”€â”€ random_traps_wrapper.py   # Wrapper randomisation piÃ¨ges
â”‚   â””â”€â”€ agents/
â”‚       â”œâ”€â”€ train_dqn.py              # Script entraÃ®nement DQN
â”‚       â”œâ”€â”€ test_agent.py             # Script test et Ã©valuation
â”‚       â””â”€â”€ q_value_extractor.py      # Extraction Q-values du rÃ©seau
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py              # Application web interactive
â”œâ”€â”€ models/
â”‚   â””â”€â”€ dqn_gridworld_final.zip       # ModÃ¨le entraÃ®nÃ© (85% accuracy)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard/                  # Logs d'entraÃ®nement

â”œâ”€â”€ requirements.txt                  # DÃ©pendances Python
â”œâ”€â”€ .gitignore                        # Fichiers exclus de Git



## ğŸ§  DÃ©tails Techniques

### Environnement GridWorld

- **Taille:** Grille 5Ã—5
- **DÃ©part:** Toujours (0,0)
- **Objectif:** Toujours (4,4)
- **PiÃ¨ges:** 1-5 placÃ©s alÃ©atoirement
- **Actions:** 4 directions (Haut, Bas, Gauche, Droite)
- **Observation:** Vecteur 14D [agent_pos, goal_pos, 5Ã—traps_pos]

### SystÃ¨me de RÃ©compenses

- Atteindre objectif: **+100**
- Tomber dans piÃ¨ge: **-10**
- Rapprochement objectif: **+0.5 Ã— distance**
- Ã‰loignement objectif: **-2.5 Ã— distance**
- ProximitÃ© piÃ¨ge (â‰¤1.0): **-2.0**
- ProximitÃ© piÃ¨ge (1.0-1.5): **-0.8**
- PÃ©nalitÃ© par pas: **-0.15**

### Configuration DQN

- **Architecture:** MLP (2 couches Ã— 64 neurones, ReLU)
- **Learning rate:** 0.0007
- **Replay buffer:** 100 000 transitions
- **Batch size:** 128
- **Gamma:** 0.97
- **Exploration:** 80% du training (epsilon 1.0 â†’ 0.15)

## ğŸ“ˆ RÃ©sultats DÃ©taillÃ©s

### Statistiques sur 20 Ã‰pisodes (3 piÃ¨ges)

- **SuccÃ¨s:** 17/20 (85%)
- **RÃ©compense moyenne:** 81.36 Â± 39.30
- **Pas moyen:** 7.3 Â± 1.8
- **Meilleure rÃ©compense:** 101.28
- **Distribution actions:** 51.4% Droite, 48.6% Bas

### Analyse Qualitative

**Comportements observÃ©s:**
- 100% des succÃ¨s atteignent l'optimal (8 pas)
- Trajectoires variÃ©es selon configuration piÃ¨ges
- Ã‰vitement anticipÃ© visible (pÃ©nalitÃ©s proximitÃ©)
- Adaptation dynamique aux obstacles

**Types d'Ã©checs:**
- ImmÃ©diats (1 pas): 2 directions optimales bloquÃ©es
- PrÃ©coces (4-5 pas): Engagement couloir bloquÃ©
- Tardifs (6-7 pas): PiÃ¨ge sur case quasi-obligatoire





