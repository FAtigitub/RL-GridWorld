"""
Tester l'agent entraÃ®nÃ© avec analyses avancÃ©es et visualisations
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import argparse
import json
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from collections import Counter
import time

from stable_baselines3 import DQN
from src.environment.gridworld import GridWorldEnv
from src.agents.q_value_extractor import QValueExtractor


def plot_trajectory(trajectory, env_info, episode_num, save_path=None):
    """Visualiser la trajectoire de l'agent sur une grille"""
    size = env_info['size']
    goal = env_info['goal_pos']
    traps = env_info['traps']
    start = trajectory[0]
    
    fig, ax = plt.subplots(figsize=(8, 8))
    
    # Dessiner la grille
    for i in range(size + 1):
        ax.axhline(i, color='gray', linewidth=0.5)
        ax.axvline(i, color='gray', linewidth=0.5)
    
    # Marquer le dÃ©part (cercle bleu)
    ax.add_patch(patches.Circle((start[0] + 0.5, start[1] + 0.5), 
                                0.3, color='cyan', zorder=3))
    ax.text(start[0] + 0.5, start[1] + 0.5, 'S', 
            ha='center', va='center', fontsize=16, fontweight='bold')
    
    # Marquer l'objectif (Ã©toile jaune)
    ax.add_patch(patches.RegularPolygon((goal[0] + 0.5, goal[1] + 0.5), 
                                        5, 0.4, color='gold', zorder=3))
    ax.text(goal[0] + 0.5, goal[1] + 0.2, 'G', 
            ha='center', va='center', fontsize=16, fontweight='bold')
    
    # Marquer les piÃ¨ges (X rouge)
    for trap in traps:
        ax.add_patch(patches.Rectangle((trap[0], trap[1]), 1, 1, 
                                       color='red', alpha=0.3, zorder=2))
        ax.text(trap[0] + 0.5, trap[1] + 0.5, 'X', 
                ha='center', va='center', fontsize=20, fontweight='bold', color='red')
    
    # Dessiner la trajectoire (ligne bleue avec points)
    x_coords = [pos[0] + 0.5 for pos in trajectory]
    y_coords = [pos[1] + 0.5 for pos in trajectory]
    ax.plot(x_coords, y_coords, 'b-', linewidth=2, alpha=0.6, zorder=1)
    ax.plot(x_coords, y_coords, 'bo', markersize=8, alpha=0.8, zorder=2)
    
    # NumÃ©roter les steps
    for i, (x, y) in enumerate(zip(x_coords, y_coords)):
        if i > 0 and i < len(trajectory) - 1:  # Skip start and end
            ax.text(x + 0.15, y + 0.15, str(i), fontsize=8, color='blue')
    
    ax.set_xlim(0, size)
    ax.set_ylim(0, size)
    ax.set_aspect('equal')
    ax.set_title(f'Trajectoire - Ã‰pisode {episode_num}\n{len(trajectory)} steps', 
                 fontsize=14, fontweight='bold')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.invert_yaxis()
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  ðŸ“Š Trajectoire sauvegardÃ©e: {save_path}")
    else:
        plt.show()
    
    plt.close()


def analyze_q_values(q_values_history, action_names):
    """Analyser l'Ã©volution des Q-values"""
    if not q_values_history:
        return None
    
    num_steps = len(q_values_history)
    num_actions = len(q_values_history[0])
    
    # CrÃ©er graphique
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # Graphique 1: Ã‰volution des Q-values
    for action_idx in range(num_actions):
        q_vals = [q_vals[action_idx] for q_vals in q_values_history]
        ax1.plot(range(num_steps), q_vals, 
                marker='o', label=action_names[action_idx], linewidth=2)
    
    ax1.set_xlabel('Step', fontsize=12)
    ax1.set_ylabel('Q-Value', fontsize=12)
    ax1.set_title('Ã‰volution des Q-Values par Action', fontsize=14, fontweight='bold')
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)
    
    # Graphique 2: Max Q-value par step
    max_q_vals = [max(q_vals) for q_vals in q_values_history]
    avg_q_vals = [np.mean(q_vals) for q_vals in q_values_history]
    
    ax2.plot(range(num_steps), max_q_vals, 'g-', marker='o', 
            label='Max Q-Value', linewidth=2)
    ax2.plot(range(num_steps), avg_q_vals, 'b--', marker='s', 
            label='Avg Q-Value', linewidth=2)
    ax2.set_xlabel('Step', fontsize=12)
    ax2.set_ylabel('Q-Value', fontsize=12)
    ax2.set_title('Q-Values Max et Moyenne', fontsize=14, fontweight='bold')
    ax2.legend(loc='best')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig


def test_agent(model_path="models/dqn_gridworld_final.zip", 
               episodes=5, 
               render=True, 
               speed=0.3,
               save_plots=False,
               save_results=False,
               analyze_qvalues=False,
               verbose=True):
    """
    Tester l'agent avec analyses avancÃ©es
    
    Args:
        model_path: Chemin vers le modÃ¨le
        episodes: Nombre d'Ã©pisodes Ã  tester
        render: Afficher le rendu Pygame
        speed: Vitesse d'animation (secondes entre steps, 0 = rapide)
        save_plots: Sauvegarder les graphiques
        save_results: Sauvegarder les rÃ©sultats JSON
        analyze_qvalues: Analyser les Q-values
        verbose: Mode verbeux
    """
    
    print("="*70)
    print("ðŸ§ª TEST AVANCÃ‰ DE L'AGENT ENTRAÃŽNÃ‰")
    print("="*70)
    
    # Charger le modÃ¨le
    print(f"\nðŸ“¦ Chargement du modÃ¨le: {model_path}")
    try:
        model = DQN.load(model_path)
    except FileNotFoundError:
        print(f"âŒ ModÃ¨le non trouvÃ©: {model_path}")
        print("ðŸ’¡ ModÃ¨les disponibles:")
        import os
        if os.path.exists("models"):
            for f in os.listdir("models"):
                if f.endswith('.zip'):
                    print(f"   - models/{f}")
        return
    
    # CrÃ©er l'environnement
    render_mode = "human" if render else None
    num_traps = 3  # Peut Ãªtre modifiÃ© (1-5 piÃ¨ges supportÃ©s)
    env = GridWorldEnv(size=5, num_traps=num_traps, render_mode=render_mode)
    extractor = QValueExtractor(model, env)
    
    print(f"\nðŸŽ® Configuration:")
    print(f"   Ã‰pisodes: {episodes}")
    print(f"   Grille: 5x5")
    print(f"   PiÃ¨ges: {num_traps}")
    print(f"   Rendu: {'ActivÃ©' if render else 'DÃ©sactivÃ©'}")
    print(f"   Vitesse: {speed}s/step" if speed > 0 else "   Vitesse: Maximum")
    print(f"   Analyse Q-values: {'Oui' if analyze_qvalues else 'Non'}")
    
    # Statistiques globales
    all_results = []
    total_rewards = []
    total_steps = []
    successes = 0
    action_counts = Counter()
    
    # CrÃ©er dossier pour les rÃ©sultats
    if save_plots or save_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = Path(f"test_results_{timestamp}")
        results_dir.mkdir(exist_ok=True)
        print(f"\nðŸ“ Dossier de rÃ©sultats: {results_dir}")
    
    print(f"\nðŸš€ Lancement des tests...\n")
    
    for episode in range(episodes):
        obs, info = env.reset()
        done = False
        episode_reward = 0
        step = 0
        
        # DonnÃ©es de l'Ã©pisode
        trajectory = [info['agent_pos'].copy()]
        actions_taken = []
        rewards_history = []
        q_values_history = []
        
        if verbose:
            print(f"{'â”€'*70}")
            print(f"ðŸ“ Ã‰pisode {episode + 1}/{episodes}")
            print(f"   DÃ©part: {info['agent_pos']} â†’ Objectif: {info['goal_pos']}")
            print(f"   PiÃ¨ges: {info['traps']}")
        
        while not done:
            # Extraire Q-values si analyse demandÃ©e
            if analyze_qvalues:
                context = extractor.extract_decision_context(obs)
                q_vals = [context['q_values'][name] for name in env.action_to_name.values()]
                q_values_history.append(q_vals)
            
            # PrÃ©dire l'action (mode dÃ©terministe)
            action, _states = model.predict(obs, deterministic=True)
            action = int(action)
            
            # ExÃ©cuter l'action
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            step += 1
            
            # Enregistrer
            trajectory.append(info['agent_pos'].copy())
            actions_taken.append(info['action_name'])
            rewards_history.append(reward)
            action_counts[info['action_name']] += 1
            
            # Afficher
            if render:
                env.render()
            
            if speed > 0:
                time.sleep(speed)
            
            if verbose:
                direction = ""
                if step > 1:
                    prev = trajectory[-2]
                    curr = trajectory[-1]
                    if curr[1] < prev[1]: direction = "â†‘"
                    elif curr[1] > prev[1]: direction = "â†“"
                    elif curr[0] < prev[0]: direction = "â†"
                    elif curr[0] > prev[0]: direction = "â†’"
                
                print(f"   Step {step:2d}: {info['action_name']:7s} {direction} â†’ "
                      f"{info['agent_pos']} | R={reward:6.2f} | Total={episode_reward:6.2f}")
        
        total_rewards.append(episode_reward)
        total_steps.append(step)
        
        # DÃ©terminer succÃ¨s
        success = episode_reward > 50
        if success:
            successes += 1
        
        if verbose:
            status = "âœ… SUCCÃˆS" if success else "âŒ Ã‰CHEC"
            print(f"\n   {status} - RÃ©compense finale: {episode_reward:.2f} ({step} steps)")
        
        # Sauvegarder trajectoire
        if save_plots:
            plot_path = results_dir / f"trajectory_ep{episode+1}.png"
            plot_trajectory(trajectory, 
                          {'size': 5, 'goal_pos': info['goal_pos'], 
                           'traps': info['traps']},
                          episode + 1, 
                          plot_path)
        
        # Analyser Q-values
        if analyze_qvalues and q_values_history:
            fig = analyze_q_values(q_values_history, env.action_to_name)
            if save_plots and fig:
                fig.savefig(results_dir / f"qvalues_ep{episode+1}.png", 
                           dpi=150, bbox_inches='tight')
                print(f"  ðŸ“Š Q-values sauvegardÃ©s: qvalues_ep{episode+1}.png")
                plt.close(fig)
            elif fig:
                plt.show()
                plt.close(fig)
        
        # Enregistrer rÃ©sultats Ã©pisode
        all_results.append({
            "episode": episode + 1,
            "success": success,
            "reward": float(episode_reward),
            "steps": step,
            "start_pos": trajectory[0],
            "goal_pos": info['goal_pos'],  # Already a list from env
            "trajectory": trajectory,
            "actions": actions_taken,
            "rewards": rewards_history
        })
    
    env.close()
    
    # Statistiques finales dÃ©taillÃ©es
    print(f"\n{'='*70}")
    print("ðŸ“Š STATISTIQUES DÃ‰TAILLÃ‰ES")
    print(f"{'='*70}")
    
    success_rate = successes / episodes * 100
    avg_reward = np.mean(total_rewards)
    avg_steps = np.mean(total_steps)
    std_reward = np.std(total_rewards)
    std_steps = np.std(total_steps)
    
    print(f"\nðŸŽ¯ Performance Globale:")
    print(f"   Taux de succÃ¨s:    {successes}/{episodes} ({success_rate:.1f}%)")
    print(f"   RÃ©compense moy.:   {avg_reward:.2f} (Â±{std_reward:.2f})")
    print(f"   Steps moyens:      {avg_steps:.1f} (Â±{std_steps:.1f})")
    print(f"   Meilleure rÃ©comp.: {max(total_rewards):.2f}")
    print(f"   Pire rÃ©compense:   {min(total_rewards):.2f}")
    print(f"   Steps min/max:     {min(total_steps)}/{max(total_steps)}")
    
    print(f"\nðŸŽ² Distribution des Actions:")
    total_actions = sum(action_counts.values())
    for action, count in sorted(action_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_actions * 100
        bar = "â–ˆ" * int(percentage / 2)
        print(f"   {action:7s}: {count:4d} ({percentage:5.1f}%) {bar}")
    
    # Graphique de synthÃ¨se
    if save_plots or episodes > 1:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
        
        # Graphique 1: RÃ©compenses par Ã©pisode
        colors = ['green' if r > 50 else 'red' for r in total_rewards]
        ax1.bar(range(1, episodes + 1), total_rewards, color=colors, alpha=0.7)
        ax1.axhline(y=50, color='orange', linestyle='--', label='Seuil succÃ¨s')
        ax1.set_xlabel('Ã‰pisode')
        ax1.set_ylabel('RÃ©compense')
        ax1.set_title('RÃ©compenses par Ã‰pisode')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Graphique 2: Steps par Ã©pisode
        ax2.plot(range(1, episodes + 1), total_steps, 'b-o', linewidth=2)
        ax2.set_xlabel('Ã‰pisode')
        ax2.set_ylabel('Nombre de Steps')
        ax2.set_title('Longueur des Ã‰pisodes')
        ax2.grid(True, alpha=0.3)
        
        # Graphique 3: Distribution des actions
        actions_list = list(action_counts.keys())
        counts_list = [action_counts[a] for a in actions_list]
        ax3.pie(counts_list, labels=actions_list, autopct='%1.1f%%', startangle=90)
        ax3.set_title('Distribution des Actions')
        
        # Graphique 4: Histogramme des rÃ©compenses
        ax4.hist(total_rewards, bins=10, color='skyblue', edgecolor='black', alpha=0.7)
        ax4.axvline(x=avg_reward, color='red', linestyle='--', linewidth=2, label=f'Moy: {avg_reward:.2f}')
        ax4.set_xlabel('RÃ©compense')
        ax4.set_ylabel('FrÃ©quence')
        ax4.set_title('Distribution des RÃ©compenses')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_plots:
            summary_path = results_dir / "summary_statistics.png"
            plt.savefig(summary_path, dpi=150, bbox_inches='tight')
            print(f"\nðŸ“Š Statistiques graphiques: {summary_path}")
        else:
            plt.show()
        plt.close()
    
    # Sauvegarder rÃ©sultats JSON
    if save_results:
        results_json = {
            "test_date": datetime.now().isoformat(),
            "model_path": model_path,
            "config": {
                "episodes": episodes,
                "grid_size": 5,
                "num_traps": num_traps
            },
            "summary": {
                "success_rate": float(success_rate),
                "avg_reward": float(avg_reward),
                "std_reward": float(std_reward),
                "avg_steps": float(avg_steps),
                "std_steps": float(std_steps),
                "min_steps": int(min(total_steps)),
                "max_steps": int(max(total_steps)),
                "best_reward": float(max(total_rewards)),
                "worst_reward": float(min(total_rewards))
            },
            "action_distribution": dict(action_counts),
            "episodes": all_results
        }
        
        json_path = results_dir / "test_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_json, f, indent=2, ensure_ascii=False)
        print(f"ðŸ’¾ RÃ©sultats JSON: {json_path}")
    
    print(f"\n{'='*70}")
    print("âœ… TESTS TERMINÃ‰S")
    print(f"{'='*70}\n")
    
    return {
        "success_rate": success_rate,
        "avg_reward": avg_reward,
        "results": all_results
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Tester l'agent DQN entraÃ®nÃ©")
    parser.add_argument("--model", type=str, default="models/dqn_gridworld_final.zip",
                       help="Chemin vers le modÃ¨le")
    parser.add_argument("--episodes", type=int, default=5,
                       help="Nombre d'Ã©pisodes Ã  tester")
    parser.add_argument("--no-render", action="store_true",
                       help="DÃ©sactiver le rendu Pygame")
    parser.add_argument("--speed", type=float, default=0.3,
                       help="Vitesse d'animation (secondes, 0=max)")
    parser.add_argument("--save-plots", action="store_true",
                       help="Sauvegarder les graphiques")
    parser.add_argument("--save-results", action="store_true",
                       help="Sauvegarder les rÃ©sultats JSON")
    parser.add_argument("--analyze-qvalues", action="store_true",
                       help="Analyser les Q-values")
    parser.add_argument("--quiet", action="store_true",
                       help="Mode silencieux")
    
    args = parser.parse_args()
    
    test_agent(
        model_path=args.model,
        episodes=args.episodes,
        render=not args.no_render,
        speed=args.speed,
        save_plots=args.save_plots,
        save_results=args.save_results,
        analyze_qvalues=args.analyze_qvalues,
        verbose=not args.quiet
    )