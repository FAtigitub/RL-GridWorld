"""
Script de test rapide pour v√©rifier qu'Ollama fonctionne
Ex√©cuter: python test_ollama_quick.py
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

try:
    import ollama
    print("‚úÖ Package ollama install√©")
except ImportError:
    print("‚ùå Package ollama non install√©")
    print("üì¶ Installer avec: pip install ollama")
    sys.exit(1)

# Tester la connexion √† Ollama
try:
    models = ollama.list()
    print(f"‚úÖ Ollama fonctionne ! Mod√®les disponibles: {len(models['models'])}")
    
    if models['models']:
        print("\nüìã Mod√®les install√©s:")
        for model in models['models']:
            print(f"   - {model['name']} ({model['size'] / 1e9:.1f}GB)")
    else:
        print("\n‚ö†Ô∏è  Aucun mod√®le install√©")
        print("üì• T√©l√©charger un mod√®le avec: ollama pull llama3.2:3b")
        
except Exception as e:
    print(f"‚ùå Ollama ne fonctionne pas: {e}")
    print("\nüì• Solutions:")
    print("   1. T√©l√©charger Ollama: https://ollama.com/download")
    print("   2. Installer et d√©marrer l'application")
    print("   3. T√©l√©charger un mod√®le: ollama pull llama3.2:3b")
    sys.exit(1)

# Test d'inf√©rence simple
print("\nüß™ Test d'inf√©rence avec le premier mod√®le disponible...")
try:
    if models['models']:
        model_name = models['models'][0]['name']
        print(f"   Mod√®le: {model_name}")
        
        response = ollama.chat(
            model=model_name,
            messages=[
                {'role': 'user', 'content': 'Dis bonjour en une phrase.'}
            ]
        )
        
        print(f"   R√©ponse: {response['message']['content']}")
        print("\n‚úÖ Ollama est pr√™t √† √™tre utilis√© !")
        print(f"\nüí° Pour l'utiliser dans le projet:")
        print(f"   from src.llm.explainer_ollama import RLExplainerOllama")
        print(f"   explainer = RLExplainerOllama(model_name='{model_name}')")
        
except Exception as e:
    print(f"‚ùå Erreur lors du test: {e}")
    sys.exit(1)
